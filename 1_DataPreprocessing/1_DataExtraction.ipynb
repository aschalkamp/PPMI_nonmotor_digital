{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import pylab as plt\n",
    "import datetime\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# packages burrowed and adapted from pypmi\n",
    "import sys\n",
    "sys.path.insert(1, '/scratch/c.c21013066/PPMI_DataPreparation/phenotype/')\n",
    "import _info2021\n",
    "import _utils\n",
    "import _loaders\n",
    "import _loadersSubitems\n",
    "import _thresholds2021 as thr\n",
    "\n",
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/scratch/c.c21013066/data/ppmi/phenotypes2021\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demographics = _loaders.load_demographics(path=path)\n",
    "# check for unreasonable data in demographics\n",
    "# go through columns and check data\n",
    "reload(thr)\n",
    "demographics_clean = demographics.copy(deep=True)\n",
    "\n",
    "for col in demographics.columns[1:]:\n",
    "    print(col)\n",
    "    info = thr.DEMOGRAPHIC_INFO[col]\n",
    "    print('    Are there NaN values? {}'.format(demographics_clean[col].isna().sum()))\n",
    "    if info['scale_level'] == 'categorical': # check if only allowed categories used\n",
    "        if col == 'diagnosis':\n",
    "            print('    How many dropped due to no category? {}'.format(demographics_clean[col].isna().sum()))\n",
    "            # we can only use data of people who belong to a category\n",
    "            demographics_clean = demographics_clean.dropna(axis='rows',how='any',subset=[col])\n",
    "        assert all(elem in info['categories']  for elem in set(demographics_clean[col].dropna()))\n",
    "    if info['scale_level'] == 'date': # check if date in range\n",
    "        # need brithdate information\n",
    "        if col == 'date_birth':\n",
    "            print('    How many dropped due to no category? {}'.format(demographics_clean[col].isna().sum()))\n",
    "            # we can only use data of people who belong to a category\n",
    "            demographics_clean = demographics_clean.dropna(axis='rows',how='any',subset=[col])\n",
    "        assert all(demographics_clean[col].dropna() >= info['min']), 'min exceeds bounds {}'.format(demographics_clean[col].min())\n",
    "        assert all(demographics_clean[col].dropna() <= info['max']), 'max exceeds bounds {}'.format(demographics_clean[col].max())\n",
    "    if info['scale_level'] == 'binomial':\n",
    "        assert  set(demographics_clean[col].dropna()) == set(info['categories'])\n",
    "    if info['scale_level'] == 'normal':\n",
    "        assert all(demographics_clean[col].dropna() >= info['min']), 'min exceeds bounds {}'.format(demographics_clean[col].min())\n",
    "        assert all(demographics_clean[col].dropna() <= info['max']), 'max exceeds bounds {}'.format(demographics_clean[col].max())\n",
    "demographics_clean.to_csv(f'{path}/demographics_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for outliers in behavioral data\n",
    "# only keep those for who we know demographics\n",
    "#print(behavior.shape,behavior_sub.shape)\n",
    "reload(thr)\n",
    "behavior = _loaders.load_behavior(path = path)\n",
    "behavior_clean = behavior[behavior['participant'].isin(demographics_clean['participant'])]\n",
    "#behavior_sub_clean = behavior_sub[behavior_sub['participant'].isin(demographics_clean['participant'])]\n",
    "#print(behavior_clean.shape,behavior_sub_clean.shape)\n",
    "print('Exclude subjects for which we do not know demographics: {}'.format(behavior.shape[0]-behavior_clean.shape[0]))\n",
    "print('Drop visits where no visit ID given: {}'.format(behavior_clean['date'].isna().sum()))\n",
    "behavior_clean = behavior_clean.dropna(axis='rows',how='all',subset=['date'])\n",
    "#behavior_sub_clean = behavior_sub_clean.dropna(axis='rows',how='all',subset=['visit'])\n",
    "#print(behavior_clean.shape,behavior_sub_clean.shape)\n",
    "# set date right\n",
    "behavior_clean.loc[np.logical_and(behavior_clean['participant']==3278,behavior_clean['visit']=='V14'),\"date\"] = pd.Timestamp(datetime.datetime(2018,12,1))\n",
    "#behavior_sub_clean.loc[np.logical_and(behavior_sub_clean['participant']==3278,behavior_sub_clean['visit']=='V14'),\"date\"] = pd.Timestamp(datetime.datetime(2018,12,1))\n",
    "\n",
    "for col in behavior_clean.columns[2:]:\n",
    "    print(col)\n",
    "    info = thr.BEHAVIORAL_INFO[col]\n",
    "    print(info)\n",
    "    print('    Are there NaN values in composite? {}'.format(behavior_clean[col].isna().sum()))\n",
    "    if info['scale_level'] == 'categorical': # check if only allowed categories used\n",
    "        assert all(elem in info['categories']  for elem in set(behavior_clean[col].dropna()))\n",
    "    if info['scale_level'] == 'date': # check if date in range\n",
    "        # need visit date info to infer age\n",
    "        if col == 'date':\n",
    "            print('    How many dropped due to no visit date? {}'.format(behavior[col].isna().sum()))\n",
    "            # we can only use data of people who belong to a category\n",
    "            behavior_clean = behavior_clean.dropna(axis='rows',how='any',subset=[col])\n",
    "            #behavior_sub_clean = behavior_sub_clean.dropna(axis='rows',how='any',subset=[col])\n",
    "        #assert all(behavior_clean[col].dropna() >= info['min']), 'min exceeds bounds {}'.format(behavior_clean[col].min())\n",
    "        #assert all(behavior_clean[col].dropna() <= info['max']), 'max exceeds bounds {}'.format(behavior_clean[col].max())\n",
    "        print('    Min exceeded? {}'.format((behavior_clean[col].dropna() < info['min']).sum()))\n",
    "        print('    Max exceeded? {}'.format((behavior_clean[col].dropna() > info['max']).sum()))\n",
    "        #print('    Min exceeded sub? {}'.format((behavior_sub_clean[col].dropna() < info['min']).sum()))\n",
    "        #print('    Max exceeded sub? {}'.format((behavior_sub_clean[col].dropna() > info['max']).sum()))\n",
    "    if info['scale_level'] == 'binomial':\n",
    "        print(set(behavior_clean[col].dropna()))\n",
    "        print(set(info['categories']))\n",
    "        #assert  set(behavior_clean[col].dropna()) == set(info['categories'])\n",
    "    if info['scale_level'] == 'normal':\n",
    "        min_exceed = behavior_clean[col].min() < info['min']\n",
    "        max_exceed = behavior_clean[col].max() < info['max']\n",
    "        print('    Min exceeded? {}'.format((behavior_clean[col].dropna() < info['min']).sum()))\n",
    "        print('    Max exceeded? {}'.format((behavior_clean[col].dropna() > info['max']).sum()))\n",
    "behavior_clean = behavior_clean.dropna(axis='rows',how='all',subset=thr.BEHAVIORAL_INFO.keys()\n",
    "                                       \n",
    "# drop benton < 0\n",
    "behavior_clean.loc[behavior_clean['benton']<0,'benton'] = 0\n",
    "# drop hvlt recognition above 12 and below 0\n",
    "behavior_clean.loc[behavior_clean['hvlt_recognition']<0,'hvlt_recognition'] = 0\n",
    "behavior_clean.loc[behavior_clean['hvlt_recognition']>12,'hvlt_recognition'] = 12\n",
    "# drop stai_state, stai_trait <0 as those all NaN\n",
    "behavior_clean.loc[behavior_clean['stai_trait']<20,'stai_trait'] = np.nan\n",
    "behavior_clean.loc[behavior_clean['stai_state']<20,'stai_state'] = np.nan\n",
    "\n",
    "behavior_clean.to_csv(\"{}/behavior_clean.csv\".format(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demographics_clean = pd.read_csv(f'{path}/demographics_clean.csv',index_col=0)\n",
    "datscan = _loaders.load_datscan_all(path=path)\n",
    "print(datscan.shape)\n",
    "datscan_clean = datscan[datscan['participant'].isin(demographics_clean['participant'])]\n",
    "print(datscan_clean.shape)\n",
    "datscan_clean = datscan_clean.dropna(axis='rows',how='all',subset=['datscan_putamen_r','datscan_caudate_r','datscan_putamen_l','datscan_caudate_l'])\n",
    "print(datscan_clean.shape)\n",
    "datscan_clean.to_csv(f'{path}/datscan_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(_loaders)\n",
    "demographics_clean = pd.read_csv(f'{path}/demographics_clean.csv',index_col=0)\n",
    "biospecimen = _loaders.load_biospecimen(path=path)\n",
    "print(biospecimen.shape)\n",
    "biospecimen_clean = biospecimen[biospecimen['participant'].isin(demographics_clean['participant'])]\n",
    "print(biospecimen_clean.shape)\n",
    "biospecimen_clean = biospecimen_clean.dropna(axis='rows',how='all',subset=['abeta_1-42','csf_alpha-synuclein','ptau','ttau','gfap','nfl'])\n",
    "print(biospecimen_clean.shape)\n",
    "biospecimen_clean = biospecimen_clean.dropna(axis='rows',how='all',subset=['date'])\n",
    "print(biospecimen_clean.shape)\n",
    "biospecimen_clean.to_csv(f'{path}/biospecimen_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine into one dataframe\n",
    "# drop visit column, instead only deal with date\n",
    "reload(_preprocess)\n",
    "covs=['gender','education','diagnosis_bl_age','time','bl_age','visit_age','diagnosis','diagnosis_age','visit']\n",
    "bio=['ptau','ttau','abeta_1-42','csf_alpha-synuclein']\n",
    "dat=['datscan_putamen_l','datscan_putamen_r','datscan_caudate_l','datscan_caudate_r']\n",
    "behavior = pd.read_csv('/scratch/c.c21013066/data/ppmi/phenotypes2021/behavior_clean.csv',index_col=0)\n",
    "targets_norm = ['updrs_i','updrs_ii','updrs_iii_OFF','updrs_iii_ON','updrs_iii_A','updrs_iii_NoMED','updrs_iv','semantic_fluency','upsit','epworth',\n",
    "               'moca','stai_trait','stai_state','rbd','systolic_bp_drop','benton','gds','lns','quip','se_adl',\n",
    "               'scopa_aut','hvlt_recall','hvlt_recognition','hvlt_retention','symbol_digit','tmtb-a']\n",
    "\n",
    "data = _preprocess.combine_phenotypes_only_closest_final('behavior_clean','/scratch/c.c21013066/data/ppmi',covs=covs,\n",
    "               clinical=targets_norm,bio=bio,dat=dat)\n",
    "data = data.rename(columns={'tmtb-a':'tmtbminusa'})\n",
    "clinical = ['updrs_i','updrs_ii','updrs_iii_OFF','updrs_iii_ON','updrs_iii_A','updrs_iii_NoMED','updrs_iv','semantic_fluency','upsit','epworth',\n",
    "               'moca','stai_trait','stai_state','rbd','systolic_bp_drop','benton','gds','lns','quip','se_adl',\n",
    "               'scopa_aut','hvlt_recall','hvlt_recognition','hvlt_retention','symbol_digit','tmtbminusa']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean data\n",
    "reload(_preprocess)\n",
    "print(\"all data\", data.shape,len(np.unique(data.index)))\n",
    "data_clean = data[data['diagnosis'].isin(['pd','hc','prod'])]\n",
    "print(\"keep only PD\", data_clean.shape,len(np.unique(data_clean.index)))\n",
    "data_clean['time_since_diagnosis'] = (data_clean['date'] - data_clean['date_diagnosis']) / np.timedelta64(1,'Y')\n",
    "data_clean = data_clean.dropna(subset=clinical,how='all',axis='rows')\n",
    "print(data_clean['diagnosis'].value_counts())\n",
    "print(\"only visits where clinical data available\",data_clean.shape,len(np.unique(data_clean.index)))\n",
    "data_clean = get_n_visits(data_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_med = pd.read_csv('/scratch/c.c21013066/data/ppmi/phenotypes2021/MDS-UPDRS_Part_III.csv',usecols=['PATNO','EVENT_ID','INFODT','PDTRTMNT'],dtype={'PATNO':'int'},parse_dates=['INFODT']).rename(columns=\n",
    "{'PATNO':'participant','EVENT_ID':'visit','INFODT':'date','PDTRTMNT':'med'})\n",
    "print(first_med[first_med['participant']==3772])\n",
    "first_med = first_med.set_index(['participant','date']).sort_index()\n",
    "no_med = first_med[np.logical_or(first_med['med']==0,first_med['med'].isna())].reset_index().groupby('participant').last()\n",
    "no_med['date'] = no_med['date'] + pd.offsets.DateOffset(years=1)\n",
    "first_med = first_med[first_med['med']==1]\n",
    "first_med = first_med.reset_index().groupby('participant').first()\n",
    "no_med = no_med[~no_med.index.isin(first_med.index)]\n",
    "first_med = pd.concat([first_med,no_med])\n",
    "\n",
    "updrs_iii_data = data_clean[np.hstack([covs,bio,dat,['date','time_since_diagnosis','updrs_iii_OFF','updrs_iii_ON','updrs_iii_A','updrs_iii_NoMED']])].dropna(axis='rows',how='all',\n",
    "                                                                                                                                        subset=['updrs_iii_OFF','updrs_iii_ON','updrs_iii_A',\n",
    "                                                                                                                                                'updrs_iii_NoMED'])\n",
    "print('How many OFF data missing', updrs_iii_data['updrs_iii_OFF'].isna().sum())\n",
    "# all non medicated people when visit is OFF\n",
    "updrs_iii_data.loc[~updrs_iii_data['updrs_iii_NoMED'].isna(),'updrs_iii_OFF'] = updrs_iii_data.loc[~updrs_iii_data['updrs_iii_NoMED'].isna(),'updrs_iii_NoMED']\n",
    "print('NoMed visits are OFF',updrs_iii_data['updrs_iii_OFF'].isna().sum())\n",
    "# all visits before first medication are OFF\n",
    "# get all dates before first med\n",
    "updrs_iii_data['before_med'] = np.nan\n",
    "for key,row in updrs_iii_data.groupby('participant')[['date']]:\n",
    "    # all visits before first medication and no ON info are OFF\n",
    "    try:\n",
    "        updrs_iii_data.loc[key,'before_med'] = np.logical_and(row['date'] < first_med.loc[key,'date'], row['updrs_iii_ON'].isna())\n",
    "    except:\n",
    "        print('problem with participant ', key)\n",
    "        updrs_iii_data.loc[key,'before_med'] = False\n",
    "updrs_iii_data.loc[updrs_iii_data['before_med'],'updrs_iii_OFF'] = updrs_iii_data.loc[updrs_iii_data['before_med'],['updrs_iii_OFF','updrs_iii_NAN','updrs_iii_NoMED']].median(axis=1)\n",
    "print('All visits before first med mention and before ON first reported are OFF',updrs_iii_data['updrs_iii_OFF'].isna().sum())\n",
    "# when there is a ON visit but no OFF, the unclear visit is the OFF one\n",
    "updrs_iii_data.loc[np.logical_and(~updrs_iii_data['updrs_iii_ON'].isna(),updrs_iii_data['updrs_iii_OFF'].isna()),'updrs_iii_OFF'] = updrs_iii_data.loc[np.logical_and(~updrs_iii_data['updrs_iii_ON'].isna(),\n",
    "updrs_iii_data['updrs_iii_OFF'].isna()),['updrs_iii_OFF','updrs_iii_A']].median(axis=1)\n",
    "print('All visits where ON is there and an undetermined, that one must be the OFF',updrs_iii_data['updrs_iii_OFF'].isna().sum())\n",
    "# when there is a OFF visit but no ON, the unclear visit is the ON one\n",
    "#updrs_iii_data.loc[np.logical_and(updrs_iii_data['updrs_iii_ON'].isna(),~updrs_iii_data['updrs_iii_OFF'].isna()),'updrs_iii_ON'] = updrs_iii_data.loc[np.logical_and(updrs_iii_data['updrs_iii_ON'].isna(),\n",
    "#~updrs_iii_data['updrs_iii_OFF'].isna()),['updrs_iii_ON','updrs_iii_NAN']].median(axis=1)\n",
    "print(updrs_iii_data['updrs_iii_OFF'].isna().sum())\n",
    "# melt\n",
    "updrs_iii_melt = pd.melt(frame=updrs_iii_data.reset_index(),id_vars=np.hstack(['participant','date','time_since_diagnosis',covs,bio,dat]),value_vars=['updrs_iii_OFF','updrs_iii_ON','updrs_iii_NAN'],var_name='ON',value_name='updrs_iii')\n",
    "updrs_iii_melt = updrs_iii_melt.dropna(how='all',axis='rows',subset=['updrs_iii'])\n",
    "# regex to transform string to identifier for ON OFF\n",
    "updrs_iii_melt['ON'] = updrs_iii_melt['ON'].replace(['updrs_iii_OFF','updrs_iii_ON','updrs_iii_A'],[False,True,np.nan])\n",
    "\n",
    "data_cleaned = pd.merge(data_clean,updrs_iii_data[['updrs_iii_ON','updrs_iii_OFF','date']],on=['participant','date'],how='outer',suffixes=['_old',\"\"])\n",
    "\n",
    "                                       \n",
    "data_cleaned['updrs_iii_OFF'] = data_cleaned['updrs_iii_OFF'].fillna(data_cleaned['updrs_iii_NoMED'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaned.to_csv('/scratch/c.c21013066/data/ppmi/phenotypes2021/clinical_progression_raw_clean.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
